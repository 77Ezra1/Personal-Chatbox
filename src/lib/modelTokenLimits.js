import { createLogger } from '../lib/logger'
const logger = createLogger('MODEL_MAX_TOKENS')

/**
 * æ¨¡å‹æœ€å¤§Tokenæ•°é…ç½®
 * 
 * é‡è¦è¯´æ˜ï¼š
 * 1. maxOutputTokensï¼šæœ€å¤§è¾“å‡ºTokenæ•°ï¼ˆmax_tokenså‚æ•°çš„æœ€å¤§å€¼ï¼‰
 * 2. contextWindowï¼šä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆè¾“å…¥+è¾“å‡ºçš„æ€»Tokenæ•°ï¼‰
 * 3. æŸäº›æ¨¡å‹çš„æœ€å¤§è¾“å‡ºæ˜¯åŠ¨æ€çš„ï¼Œå–å†³äºè¾“å…¥é•¿åº¦
 * 
 * æ•°æ®æ¥æºï¼šå„æœåŠ¡å•†å®˜æ–¹APIæ–‡æ¡£
 * æœ€åæ›´æ–°ï¼š2025å¹´10æœˆ10æ—¥
 */

export const MODEL_MAX_TOKENS = {
  // ============================================
  // OpenAI Models
  // æ¥æºï¼šhttps://platform.openai.com/docs/models
  // ============================================
  
  // GPT-5 ç³»åˆ—
  'gpt-5': {
    maxOutputTokens: 128000,
    contextWindow: 400000,
    description: 'GPT-5 - æœ€å¼ºå¤§çš„OpenAIæ¨¡å‹'
  },
  
  // GPT-4o ç³»åˆ—
  'gpt-4o': {
    maxOutputTokens: 16384,          // å®˜æ–¹ç¡®è®¤
    contextWindow: 128000,
    description: 'GPT-4o - å¤šæ¨¡æ€æ——èˆ°æ¨¡å‹'
  },
  'gpt-4o-mini': {
    maxOutputTokens: 16384,          // å®˜æ–¹ç¡®è®¤
    contextWindow: 128000,
    description: 'GPT-4o mini - å¿«é€Ÿä¸”ç»æµçš„å°å‹æ¨¡å‹'
  },
  'gpt-4o-2024-08-06': {
    maxOutputTokens: 16384,
    contextWindow: 128000,
    description: 'GPT-4o (2024-08-06)'
  },
  'gpt-4o-2024-11-20': {
    maxOutputTokens: 16384,
    contextWindow: 128000,
    description: 'GPT-4o (2024-11-20)'
  },
  'gpt-4o-mini-2024-07-18': {
    maxOutputTokens: 16384,
    contextWindow: 128000,
    description: 'GPT-4o mini (2024-07-18)'
  },
  
  // GPT-4.1 ç³»åˆ—
  'gpt-4.1': {
    maxOutputTokens: 32768,
    contextWindow: 128000,
    description: 'GPT-4.1 - æœ€æ™ºèƒ½çš„éæ¨ç†æ¨¡å‹'
  },
  'gpt-4.1-mini': {
    maxOutputTokens: 32768,
    contextWindow: 128000,
    description: 'GPT-4.1 mini'
  },
  
  // GPT-4 ç³»åˆ—
  'gpt-4': {
    maxOutputTokens: 8192,
    contextWindow: 8192,
    description: 'GPT-4 æ¨¡å‹'
  },
  'gpt-4-turbo': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'GPT-4 Turbo'
  },
  
  // GPT-3.5 ç³»åˆ—
  'gpt-3.5-turbo': {
    maxOutputTokens: 4096,
    contextWindow: 16385,
    description: 'GPT-3.5 Turbo æ¨¡å‹'
  },
  
  // Oç³»åˆ—ï¼ˆæ¨ç†æ¨¡å‹ï¼‰
  'o1': {
    maxOutputTokens: 32768,
    contextWindow: 200000,
    description: 'O1 æ¨ç†æ¨¡å‹'
  },
  'o1-mini': {
    maxOutputTokens: 65536,
    contextWindow: 128000,
    description: 'O1 Mini æ¨ç†æ¨¡å‹'
  },
  'o3-mini': {
    maxOutputTokens: 65536,
    contextWindow: 128000,
    description: 'O3 Mini æ¨ç†æ¨¡å‹'
  },

  // ============================================
  // Anthropic Claude Models
  // æ¥æºï¼šhttps://docs.claude.com/en/docs/about-claude/models/overview
  // ============================================
  
  // Claude Sonnet ç³»åˆ—ï¼ˆæœ€å¤§è¾“å‡º64Kï¼‰
  'claude-sonnet-4.5': {
    maxOutputTokens: 64000,          // å®˜æ–¹ç¡®è®¤
    contextWindow: 200000,
    description: 'Claude Sonnet 4.5'
  },
  'claude-sonnet-4': {
    maxOutputTokens: 64000,          // å®˜æ–¹ç¡®è®¤
    contextWindow: 200000,
    description: 'Claude Sonnet 4'
  },
  'claude-3.7-sonnet': {
    maxOutputTokens: 64000,          // å®˜æ–¹ç¡®è®¤
    contextWindow: 200000,
    description: 'Claude 3.7 Sonnet'
  },
  'claude-3.5-sonnet': {
    maxOutputTokens: 64000,          // å®˜æ–¹ç¡®è®¤ï¼ˆä¹‹å‰é”™è¯¯é…ç½®ä¸º8192ï¼‰
    contextWindow: 200000,
    description: 'Claude 3.5 Sonnet'
  },
  'claude-3-sonnet': {
    maxOutputTokens: 64000,          // å®˜æ–¹ç¡®è®¤ï¼ˆä¹‹å‰é”™è¯¯é…ç½®ä¸º4096ï¼‰
    contextWindow: 200000,
    description: 'Claude 3 Sonnet'
  },
  'claude-3-5-sonnet-20240620': {
    maxOutputTokens: 64000,
    contextWindow: 200000,
    description: 'Claude 3.5 Sonnet (2024-06-20)'
  },
  'claude-3-5-sonnet-20241022': {
    maxOutputTokens: 64000,
    contextWindow: 200000,
    description: 'Claude 3.5 Sonnet (2024-10-22)'
  },
  'claude-3-sonnet-20240229': {
    maxOutputTokens: 64000,
    contextWindow: 200000,
    description: 'Claude 3 Sonnet (2024-02-29)'
  },
  
  // Claude Opus ç³»åˆ—ï¼ˆæœ€å¤§è¾“å‡º32Kï¼‰
  'claude-opus-4.1': {
    maxOutputTokens: 32000,          // å®˜æ–¹ç¡®è®¤
    contextWindow: 200000,
    description: 'Claude Opus 4.1'
  },
  'claude-opus-4': {
    maxOutputTokens: 32000,          // å®˜æ–¹ç¡®è®¤
    contextWindow: 200000,
    description: 'Claude Opus 4'
  },
  'claude-3-opus': {
    maxOutputTokens: 32000,          // å®˜æ–¹ç¡®è®¤ï¼ˆä¹‹å‰é”™è¯¯é…ç½®ä¸º4096ï¼‰
    contextWindow: 200000,
    description: 'Claude 3 Opus'
  },
  'claude-3-opus-20240229': {
    maxOutputTokens: 32000,
    contextWindow: 200000,
    description: 'Claude 3 Opus (2024-02-29)'
  },
  
  // Claude Haiku ç³»åˆ—
  'claude-3.5-haiku': {
    maxOutputTokens: 8192,           // å®˜æ–¹ç¡®è®¤
    contextWindow: 200000,
    description: 'Claude 3.5 Haiku'
  },
  'claude-3-haiku': {
    maxOutputTokens: 4096,           // å®˜æ–¹ç¡®è®¤
    contextWindow: 200000,
    description: 'Claude 3 Haiku'
  },
  'claude-3-5-haiku-20241022': {
    maxOutputTokens: 8192,
    contextWindow: 200000,
    description: 'Claude 3.5 Haiku (2024-10-22)'
  },
  'claude-3-haiku-20240307': {
    maxOutputTokens: 4096,
    contextWindow: 200000,
    description: 'Claude 3 Haiku (2024-03-07)'
  },

  // ============================================
  // Google Gemini Models
  // æ¥æºï¼šhttps://ai.google.dev/gemini-api/docs/models
  // ============================================
  
  'gemini-2.5-pro': {
    maxOutputTokens: 65536,          // å®˜æ–¹ç¡®è®¤
    contextWindow: 1048576,
    description: 'Gemini 2.5 Pro'
  },
  'gemini-2.0-flash': {
    maxOutputTokens: 8192,           // å®˜æ–¹ç¡®è®¤ï¼ˆä¹‹å‰é”™è¯¯é…ç½®ä¸º65536ï¼‰
    contextWindow: 1048576,
    description: 'Gemini 2.0 Flash'
  },
  'gemini-1.5-pro': {
    maxOutputTokens: 8192,           // å®˜æ–¹ç¡®è®¤
    contextWindow: 2097152,
    description: 'Gemini 1.5 Pro'
  },
  'gemini-1.5-flash': {
    maxOutputTokens: 8192,
    contextWindow: 1048576,
    description: 'Gemini 1.5 Flash'
  },
  'gemini-pro': {
    maxOutputTokens: 8192,           // å®˜æ–¹ç¡®è®¤
    contextWindow: 1048576,
    description: 'Gemini Pro'
  },
  'gemini-pro-vision': {
    maxOutputTokens: 8192,
    contextWindow: 1048576,
    description: 'Gemini Pro Vision'
  },
  'gemini-ultra': {
    maxOutputTokens: 8192,
    contextWindow: 1048576,
    description: 'Gemini Ultra'
  },

  // ============================================
  // Moonshot (Kimi) Models
  // æ¥æºï¼šhttps://platform.moonshot.cn/docs/guide/faq
  // æ³¨æ„ï¼šæœ€å¤§è¾“å‡ºæ˜¯åŠ¨æ€çš„ï¼Œå…¬å¼ä¸ºï¼šä¸Šä¸‹æ–‡çª—å£ - è¾“å…¥Tokenæ•°
  // ============================================
  
  'moonshot-v1-8k': {
    maxOutputTokens: 8192,           // å®˜æ–¹ï¼š8*1024 - prompt_tokens
    contextWindow: 8192,
    description: 'Moonshot v1 8Kï¼ˆæœ€å¤§è¾“å‡ºåŠ¨æ€ï¼‰',
    isDynamic: true
  },
  'moonshot-v1-32k': {
    maxOutputTokens: 32768,          // å®˜æ–¹ï¼š32*1024 - prompt_tokens
    contextWindow: 32768,
    description: 'Moonshot v1 32Kï¼ˆæœ€å¤§è¾“å‡ºåŠ¨æ€ï¼‰',
    isDynamic: true
  },
  'moonshot-v1-128k': {
    maxOutputTokens: 131072,         // å®˜æ–¹ï¼š128*1024 - prompt_tokensï¼ˆä¹‹å‰é”™è¯¯é…ç½®ä¸º128000ï¼‰
    contextWindow: 131072,
    description: 'Moonshot v1 128Kï¼ˆæœ€å¤§è¾“å‡ºåŠ¨æ€ï¼‰',
    isDynamic: true
  },
  'moonshot-v1-auto': {
    maxOutputTokens: 131072,
    contextWindow: 131072,
    description: 'Moonshot v1 Autoï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰',
    isDynamic: true
  },
  'kimi-k2-0711-preview': {
    maxOutputTokens: 262144,         // 256*1024ï¼ˆä¹‹å‰é”™è¯¯é…ç½®ä¸º256000ï¼‰
    contextWindow: 262144,
    description: 'Kimi K2 (0711) Previewï¼ˆæœ€å¤§è¾“å‡ºåŠ¨æ€ï¼‰',
    isDynamic: true
  },
  'kimi-k2-0905-preview': {
    maxOutputTokens: 262144,         // å®˜æ–¹ï¼š256*1024 - prompt_tokens
    contextWindow: 262144,
    description: 'Kimi K2 (0905) Previewï¼ˆæœ€å¤§è¾“å‡ºåŠ¨æ€ï¼‰',
    isDynamic: true
  },
  'kimi-k2-turbo-preview': {
    maxOutputTokens: 262144,         // å®˜æ–¹ï¼š256*1024 - prompt_tokens
    contextWindow: 262144,
    description: 'Kimi K2 Turbo Previewï¼ˆæœ€å¤§è¾“å‡ºåŠ¨æ€ï¼‰',
    isDynamic: true
  },

  // ============================================
  // Volcano Engine (è±†åŒ…) Models
  // æ¥æºï¼šç”¨æˆ·åé¦ˆ + æœç´¢ç»“æœ
  // æ³¨æ„ï¼šæ‰€æœ‰è±†åŒ…æ¨¡å‹çš„è¾“å‡ºéƒ½é™åˆ¶åœ¨çº¦16Kï¼Œä¸ä¸Šä¸‹æ–‡çª—å£æ— å…³
  // ============================================
  
  'doubao-pro-32k': {
    maxOutputTokens: 16000,          // ç”¨æˆ·åé¦ˆï¼ˆä¹‹å‰é”™è¯¯é…ç½®ä¸º32768ï¼‰
    contextWindow: 32768,
    description: 'è±†åŒ… Pro 32Kï¼ˆè¾“å‡ºé™åˆ¶çº¦16Kï¼‰'
  },
  'doubao-pro-128k': {
    maxOutputTokens: 16000,          // ç”¨æˆ·åé¦ˆï¼ˆä¹‹å‰é”™è¯¯é…ç½®ä¸º128000ï¼‰
    contextWindow: 128000,
    description: 'è±†åŒ… Pro 128Kï¼ˆè¾“å‡ºé™åˆ¶çº¦16Kï¼‰'
  },
  'doubao-pro-256k': {
    maxOutputTokens: 16000,
    contextWindow: 256000,
    description: 'è±†åŒ… Pro 256Kï¼ˆè¾“å‡ºé™åˆ¶çº¦16Kï¼‰'
  },
  'doubao-lite-32k': {
    maxOutputTokens: 16000,          // ç”¨æˆ·åé¦ˆï¼ˆä¹‹å‰é”™è¯¯é…ç½®ä¸º32768ï¼‰
    contextWindow: 32768,
    description: 'è±†åŒ… Lite 32Kï¼ˆè¾“å‡ºé™åˆ¶çº¦16Kï¼‰'
  },
  'doubao-lite-128k': {
    maxOutputTokens: 16000,
    contextWindow: 128000,
    description: 'è±†åŒ… Lite 128Kï¼ˆè¾“å‡ºé™åˆ¶çº¦16Kï¼‰'
  },

  // ============================================
  // DeepSeek Models
  // æ¥æºï¼šhttps://api-docs.deepseek.com
  // ============================================
  
  'deepseek-chat': {
    maxOutputTokens: 8192,           // å®˜æ–¹ç¡®è®¤
    contextWindow: 64000,
    description: 'DeepSeek Chat'
  },
  'deepseek-coder': {
    maxOutputTokens: 8192,           // å®˜æ–¹ç¡®è®¤
    contextWindow: 64000,
    description: 'DeepSeek Coder'
  },
  'deepseek-reasoner': {
    maxOutputTokens: 64000,          // å®˜æ–¹ç¡®è®¤ï¼šåŒ…æ‹¬æ€è€ƒè¿‡ç¨‹
    contextWindow: 64000,
    description: 'DeepSeek Reasonerï¼ˆåŒ…æ‹¬æ€è€ƒè¿‡ç¨‹ï¼‰'
  },

  // ============================================
  // Groq Models
  // æ¥æºï¼šhttps://console.groq.com/docs/models
  // ============================================
  
  'llama-3.1-8b-instant': {
    maxOutputTokens: 131072,         // å®˜æ–¹ç¡®è®¤
    contextWindow: 131072,
    description: 'Llama 3.1 8B Instant'
  },
  'llama-3.3-70b-versatile': {
    maxOutputTokens: 32768,          // å®˜æ–¹ç¡®è®¤ï¼ˆä¹‹å‰é”™è¯¯é…ç½®ä¸º8192ï¼‰
    contextWindow: 131072,
    description: 'Llama 3.3 70B Versatile'
  },
  'llama3-70b-8192': {
    maxOutputTokens: 8192,
    contextWindow: 8192,
    description: 'Llama 3 70Bï¼ˆå¯èƒ½å·²å¼ƒç”¨ï¼‰'
  },
  'llama3-8b-8192': {
    maxOutputTokens: 8192,
    contextWindow: 8192,
    description: 'Llama 3 8Bï¼ˆå¯èƒ½å·²å¼ƒç”¨ï¼‰'
  },
  'llama-3.1-70b-versatile': {
    maxOutputTokens: 8192,
    contextWindow: 131072,
    description: 'Llama 3.1 70B Versatile'
  },
  'mixtral-8x7b-32768': {
    maxOutputTokens: 32768,
    contextWindow: 32768,
    description: 'Mixtral 8x7Bï¼ˆå¯èƒ½å·²å¼ƒç”¨ï¼‰'
  },
  'gemma-7b-it': {
    maxOutputTokens: 8192,
    contextWindow: 8192,
    description: 'Gemma 7B IT'
  },
  'gemma2-9b-it': {
    maxOutputTokens: 8192,
    contextWindow: 8192,
    description: 'Gemma2 9B IT'
  },
  'meta-llama/llama-guard-4-12b': {
    maxOutputTokens: 1024,           // å®˜æ–¹ç¡®è®¤
    contextWindow: 131072,
    description: 'Llama Guard 4 12B'
  },
  'openai/gpt-oss-120b': {
    maxOutputTokens: 65536,          // å®˜æ–¹ç¡®è®¤
    contextWindow: 131072,
    description: 'GPT-OSS 120B'
  },
  'openai/gpt-oss-20b': {
    maxOutputTokens: 65536,          // å®˜æ–¹ç¡®è®¤
    contextWindow: 131072,
    description: 'GPT-OSS 20B'
  },
  'groq/compound': {
    maxOutputTokens: 8192,           // å®˜æ–¹ç¡®è®¤
    contextWindow: 131072,
    description: 'Groq Compound'
  },
  'groq/compound-mini': {
    maxOutputTokens: 8192,           // å®˜æ–¹ç¡®è®¤
    contextWindow: 131072,
    description: 'Groq Compound Mini'
  },

  // ============================================
  // Mistral Models
  // æ¥æºï¼šhttps://docs.mistral.ai/getting-started/models/models_overview/
  // æ³¨æ„ï¼šæ ¹æ®prompthub.usï¼ŒMistral Largeçš„æœ€å¤§è¾“å‡ºæ˜¯4096
  // æ–‡æ¡£ä¸­çš„Max Tokenså¯èƒ½æŒ‡ä¸Šä¸‹æ–‡çª—å£ï¼Œéœ€è¦è¿›ä¸€æ­¥ç¡®è®¤
  // ============================================
  
  // Mistral Large ç³»åˆ—
  'mistral-large-latest': {
    maxOutputTokens: 4096,           // æ ¹æ®prompthub.usï¼ˆå¾…å®˜æ–¹ç¡®è®¤ï¼‰
    contextWindow: 128000,
    description: 'Mistral Large Latest'
  },
  'mistral-large-2411': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Mistral Large (2024-11)'
  },
  'mistral-large-2407': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Mistral Large (2024-07)'
  },
  
  // Mistral Medium ç³»åˆ—
  'mistral-medium-latest': {
    maxOutputTokens: 4096,
    contextWindow: 32768,
    description: 'Mistral Medium Latest'
  },
  'mistral-medium-2508': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Mistral Medium (2025-08)'
  },
  'mistral-medium-2505': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Mistral Medium (2025-05)'
  },
  
  // Mistral Small ç³»åˆ—
  'mistral-small-latest': {
    maxOutputTokens: 4096,
    contextWindow: 32768,
    description: 'Mistral Small Latest'
  },
  'mistral-small-2506': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Mistral Small (2025-06)'
  },
  'mistral-small-2503': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Mistral Small (2025-03)'
  },
  'mistral-small-2501': {
    maxOutputTokens: 4096,
    contextWindow: 32000,
    description: 'Mistral Small (2025-01)'
  },
  'mistral-small-2407': {
    maxOutputTokens: 4096,
    contextWindow: 32000,
    description: 'Mistral Small (2024-07)'
  },
  
  // Codestral ç³»åˆ—
  'codestral-latest': {
    maxOutputTokens: 4096,
    contextWindow: 256000,
    description: 'Codestral Latest'
  },
  'codestral-2508': {
    maxOutputTokens: 4096,
    contextWindow: 256000,
    description: 'Codestral (2025-08)'
  },
  'codestral-2501': {
    maxOutputTokens: 4096,
    contextWindow: 256000,
    description: 'Codestral (2025-01)'
  },
  
  // Ministral ç³»åˆ—
  'ministral-3b-latest': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Ministral 3B Latest'
  },
  'ministral-8b-latest': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Ministral 8B Latest'
  },
  'ministral-3b-2410': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Ministral 3B (2024-10)'
  },
  'ministral-8b-2410': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Ministral 8B (2024-10)'
  },
  
  // Pixtral ç³»åˆ—
  'pixtral-large-latest': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Pixtral Large Latest'
  },
  'pixtral-large-2411': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Pixtral Large (2024-11)'
  },
  'pixtral-12b-2409': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Pixtral 12B (2024-09)'
  },
  
  // å…¶ä»– Mistral æ¨¡å‹
  'open-mistral-nemo': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Open Mistral Nemo'
  },
  'open-mistral-7b': {
    maxOutputTokens: 4096,
    contextWindow: 32768,
    description: 'Open Mistral 7B'
  },
  'open-mixtral-8x7b': {
    maxOutputTokens: 4096,
    contextWindow: 32768,
    description: 'Open Mixtral 8x7B'
  },
  'open-mixtral-8x22b': {
    maxOutputTokens: 4096,
    contextWindow: 64000,
    description: 'Open Mixtral 8x22B'
  },

  // ============================================
  // Together AI Models
  // ============================================
  
  'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo': {
    maxOutputTokens: 8192,
    contextWindow: 131072,
    description: 'Meta Llama 3.1 70B Instruct Turbo'
  },
  'meta-llama/Meta-Llama-3-8B-Instruct': {
    maxOutputTokens: 8192,
    contextWindow: 8192,
    description: 'Meta Llama 3 8B Instruct'
  },

  // ============================================
  // å…¶ä»–æ¨¡å‹
  // ============================================
  
  // Cohere
  'command-r-plus': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Command R Plus'
  },
  'command-r': {
    maxOutputTokens: 4096,
    contextWindow: 128000,
    description: 'Command R'
  },
  'command': {
    maxOutputTokens: 4096,
    contextWindow: 4096,
    description: 'Command'
  },
  'command-light': {
    maxOutputTokens: 4096,
    contextWindow: 4096,
    description: 'Command Light'
  },
  
  // Meta Llamaï¼ˆé€šè¿‡å…¶ä»–å¹³å°ï¼‰
  'llama-2-70b-chat': {
    maxOutputTokens: 4096,
    contextWindow: 4096,
    description: 'Llama 2 70B Chat'
  },
  'llama-2-13b-chat': {
    maxOutputTokens: 4096,
    contextWindow: 4096,
    description: 'Llama 2 13B Chat'
  },
  'llama-2-7b-chat': {
    maxOutputTokens: 4096,
    contextWindow: 4096,
    description: 'Llama 2 7B Chat'
  }
}

/**
 * è·å–æ¨¡å‹çš„æœ€å¤§è¾“å‡ºTokenæ•°
 * @param {string} modelName - æ¨¡å‹åç§°
 * @returns {number} æœ€å¤§è¾“å‡ºTokenæ•°ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›é»˜è®¤å€¼4096
 */
export function getModelMaxTokens(modelName) {
  const config = MODEL_MAX_TOKENS[modelName]
  return config?.maxOutputTokens ?? 4096
}

/**
 * è·å–æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°
 * @param {string} modelName - æ¨¡å‹åç§°
 * @returns {number} ä¸Šä¸‹æ–‡çª—å£å¤§å°
 */
export function getModelContextWindow(modelName) {
  const config = MODEL_MAX_TOKENS[modelName]
  return config?.contextWindow ?? 4096
}

/**
 * è·å–æ¨¡å‹çš„æè¿°ä¿¡æ¯
 * @param {string} modelName - æ¨¡å‹åç§°
 * @returns {string} æ¨¡å‹æè¿°
 */
export function getModelDescription(modelName) {
  const config = MODEL_MAX_TOKENS[modelName]
  return config?.description ?? modelName
}

/**
 * æ£€æŸ¥æ¨¡å‹çš„æœ€å¤§è¾“å‡ºæ˜¯å¦æ˜¯åŠ¨æ€çš„
 * @param {string} modelName - æ¨¡å‹åç§°
 * @returns {boolean} æ˜¯å¦åŠ¨æ€
 */
export function isModelDynamic(modelName) {
  const config = MODEL_MAX_TOKENS[modelName]
  return config?.isDynamic ?? false
}

/**
 * æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæŒ‡å®šçš„Tokenæ•°
 * @param {string} modelName - æ¨¡å‹åç§°
 * @param {number} tokens - Tokenæ•°
 * @returns {boolean} æ˜¯å¦æ”¯æŒ
 */
export function isTokenCountSupported(modelName, tokens) {
  const maxTokens = getModelMaxTokens(modelName)
  return tokens <= maxTokens
}

/**
 * è·å–æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
 * @returns {string[]} æ¨¡å‹åç§°åˆ—è¡¨
 */
export function getSupportedModels() {
  return Object.keys(MODEL_MAX_TOKENS)
}

/**
 * æœ€å¤§Tokenæ•°çš„ä¼˜åŠ£åŠ¿è¯´æ˜
 */
export const MAX_TOKENS_PROS_CONS = {
  zh: {
    maxTokens: {
      title: 'è®¾ç½®ä¸º"æ— é™åˆ¶"',
      pros: {
        title: 'ä¼˜åŠ¿',
        items: [
          'ä½¿ç”¨æ¨¡å‹æ”¯æŒçš„æœ€å¤§è¾“å‡ºTokenæ•°',
          'ç”Ÿæˆæ›´é•¿ã€æ›´å®Œæ•´çš„å›ç­”',
          'é€‚åˆéœ€è¦è¯¦ç»†è§£é‡Šæˆ–é•¿ç¯‡å†…å®¹çš„åœºæ™¯',
          'å‡å°‘å› Tokené™åˆ¶å¯¼è‡´çš„å†…å®¹æˆªæ–­',
          'æä¾›æ›´å…¨é¢çš„ä¿¡æ¯å’Œåˆ†æ'
        ]
      },
      cons: {
        title: 'åŠ£åŠ¿',
        items: [
          'æ¶ˆè€—æ›´å¤šAPIé…é¢å’Œè´¹ç”¨',
          'å“åº”æ—¶é—´å¯èƒ½æ›´é•¿',
          'å¯èƒ½äº§ç”Ÿå†—ä½™æˆ–ä¸å¿…è¦çš„å†…å®¹',
          'åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å½±å“å“åº”é€Ÿåº¦'
        ]
      }
    },
    defaultTokens: {
      title: 'è®¾ç½®ä¸ºå…·ä½“æ•°å€¼',
      pros: {
        title: 'ä¼˜åŠ¿',
        items: [
          'ç²¾ç¡®æ§åˆ¶è¾“å‡ºé•¿åº¦',
          'èŠ‚çœAPIé…é¢å’Œè´¹ç”¨',
          'å“åº”é€Ÿåº¦æ›´å¿«',
          'å†…å®¹æ›´åŠ ç²¾ç‚¼å’Œèšç„¦',
          'é€‚åˆå¿«é€Ÿé—®ç­”å’Œç®€çŸ­å›å¤'
        ]
      },
      cons: {
        title: 'åŠ£åŠ¿',
        items: [
          'å¯èƒ½æ— æ³•ç”Ÿæˆå®Œæ•´çš„é•¿ç¯‡å†…å®¹',
          'å¤æ‚é—®é¢˜çš„å›ç­”å¯èƒ½è¢«æˆªæ–­',
          'éœ€è¦å¤šè½®å¯¹è¯æ‰èƒ½è·å–å®Œæ•´ä¿¡æ¯',
          'ä¸é€‚åˆéœ€è¦è¯¦ç»†åˆ†æçš„åœºæ™¯'
        ]
      }
    },
    recommendation: {
      title: 'ä½¿ç”¨å»ºè®®',
      content: 'æ ¹æ®æ‚¨çš„ä½¿ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„Tokenæ•°ï¼š\n\nâ€¢ ç®€çŸ­é—®ç­”ã€æ—¥å¸¸å¯¹è¯ï¼šä½¿ç”¨è¾ƒå°å€¼ï¼ˆ1024-2048ï¼‰\nâ€¢ ä»£ç ç”Ÿæˆã€æŠ€æœ¯æ–‡æ¡£ï¼šä½¿ç”¨ä¸­ç­‰å€¼ï¼ˆ4096-8192ï¼‰\nâ€¢ é•¿ç¯‡æ–‡ç« ã€è¯¦ç»†åˆ†æï¼šä½¿ç”¨"æ— é™åˆ¶"\nâ€¢ æˆæœ¬æ•æ„Ÿåœºæ™¯ï¼šä½¿ç”¨è¾ƒå°å€¼å¹¶æ ¹æ®éœ€è¦è°ƒæ•´\n\nğŸ’¡ æç¤ºï¼šé€‰æ‹©"æ— é™åˆ¶"æ—¶ï¼Œç³»ç»Ÿä¸ä¼šä¼ é€’max_tokenså‚æ•°ç»™APIï¼Œè®©æ¨¡å‹ä½¿ç”¨å…¶æ”¯æŒçš„æœ€å¤§è¾“å‡ºTokenæ•°ã€‚'
    }
  },
  en: {
    maxTokens: {
      title: 'Set to "Unlimited"',
      pros: {
        title: 'Advantages',
        items: [
          'Use model\'s maximum supported output tokens',
          'Generate longer and more complete responses',
          'Suitable for scenarios requiring detailed explanations or long-form content',
          'Reduce content truncation due to token limits',
          'Provide more comprehensive information and analysis'
        ]
      },
      cons: {
        title: 'Disadvantages',
        items: [
          'Consume more API quota and costs',
          'Response time may be longer',
          'May produce redundant or unnecessary content',
          'May affect response speed in some cases'
        ]
      }
    },
    defaultTokens: {
      title: 'Set to Specific Value',
      pros: {
        title: 'Advantages',
        items: [
          'Precise control over output length',
          'Save API quota and costs',
          'Faster response speed',
          'More concise and focused content',
          'Suitable for quick Q&A and short replies'
        ]
      },
      cons: {
        title: 'Disadvantages',
        items: [
          'May not generate complete long-form content',
          'Answers to complex questions may be truncated',
          'Multiple rounds of conversation needed for complete information',
          'Not suitable for scenarios requiring detailed analysis'
        ]
      }
    },
    recommendation: {
      title: 'Recommendations',
      content: 'Choose appropriate token count based on your use case:\n\nâ€¢ Short Q&A, casual conversation: Use smaller values (1024-2048)\nâ€¢ Code generation, technical docs: Use medium values (4096-8192)\nâ€¢ Long articles, detailed analysis: Use "Unlimited"\nâ€¢ Cost-sensitive scenarios: Use smaller values and adjust as needed\n\nğŸ’¡ Tip: When "Unlimited" is selected, the system will not pass the max_tokens parameter to the API, allowing the model to use its maximum supported output tokens.'
    }
  }
}

